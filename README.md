# LLM-catalyst

Official pytorch implementation of **Two Optimizers Are Better Than One: LLM Catalyst for Enhancing Gradient-Based Optimization**.

<!-- [[arxiv](https://arxiv.org/)]  -->

<!-- [![arXiv](https://img.shields.io/badge/arXiv-2403.05438-b31b1b.svg)](https://arxiv.org/abs/) -->

For more details, please see the [paper](https://arxiv.org/).

Contact us with zixian_guo@foxmail.com

<center>
<img src="./assets/figure_m.png">
</center>


## Citation

```bibtex

```


## Acknowledgement

We borrows code from [CoOp](https://github.com/KaiyangZhou/CoOp), [Dassl](https://github.com/KaiyangZhou/Dassl.pytorch) and [P-tuning](https://github.com/THUDM/P-tuning-v2), which are great repositories and we encourage you to check them out and cite them in your work.

